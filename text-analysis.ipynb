{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar los datos\n",
    "df_original = pd.read_csv('data/tipo1_entrenamiento_estudiantes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_original.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_original.copy()\n",
    "df_stats['Word_Count'] = df_stats['Review'].apply(lambda x: len(x.split()))\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_stats = df_stats['Word_Count'].describe()\n",
    "word_count_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay registros nulos, los datos son completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de variable objetivo\n",
    "df_original['Class'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los valores de la columna 'Class' son válidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 71 registro duplicados, estos se deben eliminar del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = df_original[df_original.duplicated()]\n",
    "duplicated_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento y Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar las filas duplicadas\n",
    "df_prep = df_original.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: balancear las clases (variable objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimnar filas con texto en otros idiomas\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language == 'es'  # Check if language is Spanish\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_prep[df_prep['Review'].apply(detect_language)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: guardar datos despues de transformaciones de limpieza\n",
    "# df_prep.to_csv('data/tipo1_entrenamiento_estudiantes_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisón en entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_prep[\"Review\"], df_prep[\"Class\"], test_size = 0.3, stratify = df_prep[\"Class\"], random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización / Vectorización\n",
    "\n",
    "\"Feature engineering\"\n",
    "\n",
    "* Bag of Words / Count Tokenizer\n",
    "* Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # eliminar caracteres numericos\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "\n",
    "    # eliminar puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # eliminar caracters especiales\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons TODO: not all emojis included here\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')  \n",
    "     \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # eliminar los stems de las palabras\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(stop_words=list(stop_words), tokenizer=tokenize_text)\n",
    "tfidf_vectorizer = TfidfVectorizer( stop_words=list(stop_words), tokenizer=tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_sklearn = pd.DataFrame(X_train_bow.toarray(),columns=bow_vectorizer.get_feature_names_out())\n",
    "df_bow_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfid_sklearn = pd.DataFrame(X_train_tfidf.toarray(),columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df_tfid_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de palabras con baja frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el vocabulario\n",
    "vocabulario = bow_vectorizer.vocabulary_\n",
    "\n",
    "# Obtener la suma de las frecuencias de cada palabra en todo el conjunto de datos\n",
    "frecuencia_total = X_train_bow.sum(axis=0)\n",
    "# Crear un diccionario que mapea cada palabra a su frecuencia total\n",
    "frecuencias = {palabra: frecuencia_total[0, indice] for palabra, indice in vocabulario.items()}\n",
    "\n",
    "# Convertir el diccionario de frecuencias en un DataFrame\n",
    "df_frecuencias = pd.DataFrame(list(frecuencias.items()), columns=['Palabra', 'Frecuencia'])\n",
    "\n",
    "# Ordenar el DataFrame por frecuencia de forma descendente\n",
    "df_frecuencias = df_frecuencias.sort_values(by='Frecuencia', ascending=False)\n",
    "# Imprimir las frecuencias de cada palabra\n",
    "df_frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_palabras_altafrec = df_frecuencias[df_frecuencias['Frecuencia'] > 50]\n",
    "df_palabras_altafrec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df_palabras_altafrec['Frecuencia'])\n",
    "plt.title('Boxplot de la frecuencia de las palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las palabras de alta frecuencia como una lista\n",
    "palabras_altafrecuencia = df_palabras_altafrec['Palabra'].tolist()\n",
    "\n",
    "# Filtrar las columnas que coinciden con las palabras de alta frecuencia\n",
    "df_bow_sklearn_filtrado = df_bow_sklearn[palabras_altafrecuencia]\n",
    "\n",
    "len(df_bow_sklearn_filtrado.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de nuevo a un array de NumPy\n",
    "X_train_bow_filtered = df_bow_sklearn_filtrado.to_numpy()\n",
    "X_train_bow_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [nombre algoritmo]\n",
    "\n",
    "Desarrollado por:\n",
    "\n",
    "[descripción]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Desarrollado por: Maria Castro Iregui\n",
    "\n",
    "[descripción]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Current time:\", current_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento sin filtro de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_bow, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = np.exp(naive_bayes.feature_log_prob_)[1,:] - np.exp(naive_bayes.feature_log_prob_)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(naive_bayes.feature_log_prob_[1], index = bow_vectorizer.vocabulary_).sort_values().tail(20).plot.barh(figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_naive_predict = naive_bayes.predict(X_train_bow)\n",
    "y_test_naive_predict = naive_bayes.predict(bow_vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clases', len(naive_bayes.classes_))\n",
    "print('Etiquetas:', naive_bayes.classes_)\n",
    "print('Features:', len(naive_bayes.feature_log_prob_[0]))\n",
    "print('Features relevantes', np.count_nonzero(naive_bayes.feature_log_prob_))\n",
    "print('Prior probabilities:', naive_bayes.class_log_prior_)\n",
    "print('Feature log probabilities:', naive_bayes.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_naive_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", naive_bayes.score(X_train, y_train))\n",
    "print(\"precision:\", precision_score(y_train, y_train_naive_predict, average='macro'))\n",
    "print(\"recall:\", recall_score(y_train, y_train_naive_predict, average='macro'))\n",
    "print(\"f1:\", f1_score(y_train, y_train_naive_predict, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_naive_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", naive_bayes.score(tfidf_vectorizer.transform(X_test), y_test))\n",
    "print(\"precision:\", precision_score(y_test, y_test_naive_predict, average='macro'))\n",
    "print(\"recall:\", recall_score(y_test, y_test_naive_predict, average='macro'))\n",
    "print(\"f1:\", f1_score(y_test, y_test_naive_predict, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con filtro de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_filtered = MultinomialNB()\n",
    "naive_bayes_filtered.fit(X_train_bow_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = np.exp(naive_bayes_filtered.feature_log_prob_)[1,:] - np.exp(naive_bayes_filtered.feature_log_prob_)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(naive_bayes_filtered.feature_log_prob_[1], index = df_palabras_altafrec['Palabra']).sort_values().tail(20).plot.barh(figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_naive_predict = naive_bayes_filtered.predict(X_train_bow)\n",
    "\n",
    "#  la eliminacion frecuencias bajas en text\n",
    "\n",
    "# X_test as df\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "df_X_test_bow = pd.DataFrame(X_test_bow.toarray(),columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "df_X_test_bow_filtrado = df_X_test_bow[palabras_altafrecuencia]\n",
    "X_test_bow_filtrado = df_X_test_bow_filtrado.to_numpy()\n",
    "\n",
    "# Filtrar las columnas que coinciden con las palabras de alta frecuencia\n",
    "\n",
    "y_test_naive_predict = naive_bayes_filtered.predict(X_test_bow_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clases', len(naive_bayes_filtered.classes_))\n",
    "print('Etiquetas:', naive_bayes_filtered.classes_)\n",
    "print('Features:', len(naive_bayes_filtered.feature_log_prob_[0]))\n",
    "print('Features relevantes', np.count_nonzero(naive_bayes_filtered.feature_log_prob_))\n",
    "print('Prior probabilities:', naive_bayes_filtered.class_log_prior_)\n",
    "print('Feature log probabilities:', naive_bayes_filtered.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_naive_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", naive_bayes_filtered.score(X_train_bow_filtered, y_train))\n",
    "print(\"precision:\", precision_score(y_train, y_train_naive_predict, average='macro'))\n",
    "print(\"recall:\", recall_score(y_train, y_train_naive_predict, average='macro'))\n",
    "print(\"f1:\", f1_score(y_train, y_train_naive_predict, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", naive_bayes_filtered.score(X_test_bow_filtrado, y_test))\n",
    "print(\"precision:\", precision_score(y_test, y_test_naive_predict, average='macro'))\n",
    "print(\"recall:\", recall_score(y_test, y_test_naive_predict, average='macro'))\n",
    "print(\"f1:\", f1_score(y_test, y_test_naive_predict, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [nombre algoritmo]\n",
    "\n",
    "Desarrollado por:\n",
    "\n",
    "[descripción]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
