{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay, RocCurveDisplay,\n",
    "    roc_auc_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "SEED=42\n",
    "\n",
    "#Pipeline\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar los datos\n",
    "df_original = pd.read_csv('../data/tipo1_entrenamiento_estudiantes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar las filas duplicadas\n",
    "df_prep = df_original.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_prep[\"Review\"], df_prep[\"Class\"], test_size = 0.3, stratify = df_prep[\"Class\"], random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # cargar las stopwords\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    # convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # eliminar caracteres numericos\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # eliminar puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # eliminar caracters especiales\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') \n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # eliminar las stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # eliminar los stems de las palabras\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "x_test_vec=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', max_iter=1000)  # Increase max_iter for convergence\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: ['pesim', 'peor', 'mal', 'suci', 'horribl', 'terribl', 'cobr', 'rob', 'habi', 'pag']\n",
      "Score 2: ['mal', 'decepcion', 'habit', 'asign', 'pobr', 'nadi', 'esper', 'dolar', 'ped', 'oli']\n",
      "Score 3: ['bastant', 'normal', 'embarg', 'aunqu', 'falt', 'monument', 'men', 'demasi', 'viv', 'general']\n",
      "Score 4: ['buen', 'limpi', 'excelent', 'comod', 'agrad', 'disfrut', 'ciud', 'ques', 'centr', 'hermos']\n",
      "Score 5: ['excelent', 'delici', 'recomend', 'increibl', 'encant', 'hermos', 'atencion', 'maravill', 'color', 'perfect']\n",
      "\n",
      "Evaluation Metrics:\n",
      "Train Accuracy: 0.7997070670084219\n",
      "Train Recall (Weighted): 0.7997070670084219\n",
      "Train F1-Score (Weighted): 0.7983510861142579\n",
      "Test Accuracy: 0.4884713919726729\n",
      "Test Recall (Weighted): 0.4884713919726729\n",
      "Test F1-Score (Weighted): 0.4791736134413112\n",
      "Confusion Matrix: [[ 78  86  38  15  20]\n",
      " [ 46 133 111  35  23]\n",
      " [ 11  57 162 152  84]\n",
      " [  4  24  81 254 226]\n",
      " [  1   4  31 149 517]]\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Definir la función de tokenización\n",
    "def tokenize_text(text):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def create_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=tokenize_text)),\n",
    "        ('model', LogisticRegression(multi_class='multinomial', max_iter=1000))\n",
    "    ])\n",
    "    dump(pipeline, 'pipeline.joblib')\n",
    "    return pipeline\n",
    "\n",
    "def train_evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test):\n",
    "    # Entrenar el pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    dump(pipeline, 'pipeline.joblib')\n",
    "\n",
    "    # Predecir en los conjuntos de entrenamiento y prueba\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calcular métricas de evaluación para el conjunto de entrenamiento\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    # Calcular métricas de evaluación para el conjunto de prueba\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    # Calcular la matriz de confusión para el conjunto de prueba\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Devolver las métricas de evaluación\n",
    "    return {\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Train Recall (Weighted)': train_recall,\n",
    "        'Train F1-Score (Weighted)': train_f1,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Recall (Weighted)': test_recall,\n",
    "        'Test F1-Score (Weighted)': test_f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def get_top_features(pipeline):\n",
    "    # Obtener el vectorizador TF-IDF del pipeline\n",
    "    tfidf_vectorizer = pipeline.named_steps['tfidf']\n",
    "    # Obtener los nombres de las características\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    # Obtener el modelo del pipeline\n",
    "    model = pipeline.named_steps['model']\n",
    "    coefficients = model.coef_\n",
    "\n",
    "    # Inicializar la lista de palabras más importantes por score\n",
    "    top_words_by_score = []\n",
    "\n",
    "    # Obtener las palabras más influyentes para cada score\n",
    "    for i, score_coefficients in enumerate(coefficients):\n",
    "        sorted_indices = np.argsort(score_coefficients)\n",
    "        top_indices = sorted_indices[-10:]  # Obtener los índices de las 10 palabras principales\n",
    "        top_indices = top_indices[::-1]  # Invertir para obtener las palabras con los coeficientes más altos primero\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "        top_words_by_score.append(top_words)\n",
    "\n",
    "    return top_words_by_score\n",
    "\n",
    "\n",
    "#Cargar datos\n",
    "df_original = pd.read_csv('../data/tipo1_entrenamiento_estudiantes.csv')\n",
    "df_prep = df_original.drop_duplicates()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_prep[\"Review\"], df_prep[\"Class\"], test_size=0.3, stratify=df_prep[\"Class\"], random_state=42)\n",
    "\n",
    "# Utilizar la función para evaluar el pipeline\n",
    "pipeline = create_pipeline()\n",
    "evaluation_metrics = train_evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test)\n",
    "top_features = get_top_features(pipeline)\n",
    "for i, score_words in enumerate(top_features, start=1):\n",
    "    print(f\"Score {i}: {score_words}\")\n",
    "\n",
    "# Imprimir las métricas de evaluación\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055    Desde que llegamos a la ciudad, un taxista nos...\n",
      "7365    Compramos los boletos de 8 CUC, unos de 8 Euro...\n",
      "4984    Llegamos al hotel a las 1:30 pm que es tempran...\n",
      "6809    Pasamos aqui una noche con mi sra. para buscar...\n",
      "3447    No era mal pero las carnes que comimos eran de...\n",
      "Name: Review, dtype: object\n",
      "\n",
      "Predictions:\n",
      "Prediction 1: Desde que llegamos a la ciudad, un taxista nos recomendó este lugar. No sólo el cabrito es exquisito sino también sus cortes. La atención es magnífica.. uy familiar. los platillos son abundantes y tienen una deliciosa  y fresca barra de ensaladas - Predicted Class: 5\n",
      "Prediction 2: Compramos los boletos de 8 CUC, unos de 8 Euros para ver el espectaculo del cañonazo que duro 15 minutos. En otro lugares por el mismo precio puedes ver todo un espectaculo mejor organizado y con mas cultura. El lugar es muy bonito con una vista sensacional a la habana Vieja - Predicted Class: 5\n",
      "Prediction 3: Llegamos al hotel a las 1:30 pm que es temprano para las 3 pm. Nos dijeron que volviésemos a las 3 de la tarde ya no tenían una habitación disponible todavía. Nos fuimos y pasamos tiempo en el restaurante cercano. A las 3 de la tarde nos dijeron que una habitación todavía no están disponibles aunque habíamos reservado previamente nuestra estancia y pagamos cuando llegamos a la 1:30 de la tarde. Esperamos en la zona del vestíbulo. Mi marido fuimos dos veces a la barra después de 10, luego 25 minutos más tarde. Finalmente, le dijeron que la habitación estuviera lista la segunda vez. La habitación estaba razonablemente limpia. La cama era incómoda mientras intentaba dormir; y, mantiene puertas cuando cuando estábamos sentado en él viendo la tele. Mi marido finalmente tuve que poner… - Predicted Class: 2\n",
      "Prediction 4: Pasamos aqui una noche con mi sra. para buscar otro lugar donde hospedarnos. La verdad que el hotel ha conocido mejores épocas, las camas incómodas, el agua del baño mal... lo único para destacar el bar del lobby donde cuando estuvimos hicieron un show flamenco muy interesante y el barman que era un master en polìtica... - Predicted Class: 2\n",
      "Prediction 5: No era mal pero las carnes que comimos eran demaciado cocinadas y el congris no me parecio fresco. Por el restp todo me pareció bastante bueno y agradable. Yo pero la proxima vez pruevo otro puesto. No por nada... - Predicted Class: 3\n"
     ]
    }
   ],
   "source": [
    "def predict_with_pipeline(input_df):\n",
    "    # Cargar el pipeline desde el archivo joblib\n",
    "    pipeline = load('pipeline.joblib')\n",
    "    # Realizar la predicción utilizando el pipeline\n",
    "    predictions = pipeline.predict(input_df)\n",
    "\n",
    "    # Devolver las predicciones\n",
    "    return predictions\n",
    "\n",
    "# Llamada a la función para obtener las predicciones\n",
    "input_df = pd.read_csv('../data/tipo1_entrenamiento_estudiantes.csv')\n",
    "# Seleccione 5 filas para predecir aleatoriamente\n",
    "input_df_sample = input_df.sample(5)['Review']\n",
    "print(input_df_sample)\n",
    "predictions = predict_with_pipeline(input_df_sample)\n",
    "\n",
    "# Imprimir las predicciones\n",
    "print(\"\\nPredictions:\")\n",
    "for i, prediction in enumerate(predictions, start=1):\n",
    "    print(f\"Prediction {i}: {input_df_sample.iloc[i-1]} - Predicted Class: {prediction}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
